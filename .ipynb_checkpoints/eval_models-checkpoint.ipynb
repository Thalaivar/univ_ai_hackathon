{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import *\n",
    "from main import data_preprocess, SINGLE_TRANSFORMS\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import catboost as cboost\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(preds, best_file):\n",
    "    return roc_auc_score(pd.read_csv(best_file)[\"risk_flag\"].values, preds)\n",
    "\n",
    "def eval_on_original(model, targets=\"risk_flag\", data_params={}, fit_params={}, par_mod=None, n=1, **params):\n",
    "    model = clone(model)\n",
    "    \n",
    "    if par_mod:\n",
    "        params = par_mod(params)\n",
    "    model.set_params(**params)\n",
    "    \n",
    "    df_train, df_test = data_preprocess(**data_params)\n",
    "    \n",
    "    res = []\n",
    "    for _ in range(n):\n",
    "        X, y = df_train.drop([\"id\", targets], axis=1), df_train[targets].values\n",
    "        model.fit(X, y, **fit_params)\n",
    "    \n",
    "        preds = model.predict(df_test.drop(\"id\", axis=1))\n",
    "        res.append(evaluate_predictions(preds, \"./BEST.csv\"))\n",
    "        \n",
    "    return sum(res) / len(res)\n",
    "\n",
    "\n",
    "def tuning_w_original(model, pbounds, targets=\"risk_flag\", data_params={}, fit_params={}, par_mod=None, **opt_kwargs):\n",
    "    eval_fn = partial(eval_on_original, model, targets, data_params, fit_params, par_mod)\n",
    "    BO = BayesianOptimization(eval_fn, pbounds)\n",
    "    BO.maximize(**opt_kwargs)\n",
    "    return BO\n",
    "\n",
    "def tuning_super_learner(model, pbounds, fit_params={}, par_mod=None, n=5, **opt_kwargs):\n",
    "    df_train = pd.read_csv(\"./ensemble_files/ensemble_train.csv\")\n",
    "    df_test = pd.read_csv(\"./ensemble_files/ensemble_test.csv\")\n",
    "\n",
    "    def eval_fn(model, df_train, df_test, fit_params, par_mod, **params):\n",
    "        model = clone(model)\n",
    "        params = par_mod(params)\n",
    "        model.set_params(**params)\n",
    "\n",
    "        res = []\n",
    "        for _ in range(n):\n",
    "            model.fit(df_train.drop([\"id\", \"targets\"], axis=1), df_train[\"targets\"].values, **fit_params)\n",
    "            preds = model.predict(df_test.drop(\"id\", axis=1))\n",
    "            res.append(evaluate_predictions(preds, \"./BEST.csv\"))\n",
    "        \n",
    "        return sum(res)/len(res)\n",
    "    \n",
    "    eval_fn = partial(eval_fn, model, df_train, df_test, fit_params, par_mod)\n",
    "    superBO = BayesianOptimization(eval_fn, pbounds)\n",
    "    superBO.maximize(**opt_kwargs)\n",
    "    return superBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning on Original Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xgb_tuning():\n",
    "    model = XGBClassifier(\n",
    "        tree_method=\"gpu_hist\", \n",
    "        n_estimators=10000,\n",
    "        eval_metric=\"logloss\", \n",
    "        scale_pos_weight=19.35097,\n",
    "        max_depth=8,\n",
    "        min_child_weight=4.525,\n",
    "        gamma=0.938358\n",
    "    )\n",
    "    \n",
    "    pbounds = {\n",
    "        \"learning_rate\": (-4, -2),\n",
    "    }\n",
    "    \n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"reg_alpha\", \"learning_rate\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "    \n",
    "    return tuning_w_original(model, pbounds, par_mod=par_mod, init_points=20, n_iter=40)\n",
    "\n",
    "xgbBO = xgb_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_tuning():\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        device=\"gpu\",\n",
    "        boosting_type=\"goss\",\n",
    "        num_leaves=229,\n",
    "        max_depth=7,\n",
    "        min_child_weight=7.5546,\n",
    "        max_bin=96,\n",
    "        scale_pos_weight=13.56929,\n",
    "        colsample_bytree=0.95781,\n",
    "        subsample=0.74331,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\", \"max_bin\", \"num_leaves\", \"bagging_freq\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"lambda_l1\", \"lambda_l2\", \"learning_rate\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "    \n",
    "    pbounds = {\n",
    "        \"lambda_l1\": (-5, 1),\n",
    "        \"lambda_l2\": (-5, 1),\n",
    "#         \"reg_alpha\": (-5, 1)\n",
    "    }\n",
    "    \n",
    "    return tuning_w_original(model, pbounds, par_mod=par_mod, init_points=20, n_iter=40)\n",
    "lgbmBO = lgbm_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_tuning():\n",
    "    cat_features = [\"house_ownership\", \"car_ownership\", \"married\", \"city\", \"profession\", \"state\", \"age\", \"experience\", \"income\"]\n",
    "    model = CatBoostClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        loss_function=\"Logloss\",\n",
    "        boosting_type=\"Ordered\",\n",
    "        eval_metric=\"AUC\",\n",
    "        cat_features=cat_features,\n",
    "        task_type=\"GPU\",\n",
    "        max_depth=5,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\", \"max_bin\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"l2_leaf_reg\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "   \n",
    "    pbounds = {\n",
    "#         \"random_strength\": (0, 20),\n",
    "#         \"subsample\": (0.1, 1),\n",
    "#         \"scale_pos_weight\": (5, 20),\n",
    "#         \"l2_leaf_reg\": (-5, 1),\n",
    "#         \"bagging_temperature\": (0.1, 1)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    data_params = {\"drop_cols\": {}, \"transforms\": [(catboost_dataset, {\"cols\": cat_features})], \"no_encode\": True}\n",
    "    return tuning_w_original(model, pbounds, par_mod=par_mod, data_params=data_params, init_points=20, n_iter=40)\n",
    "cboostBO = catboost_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8200822481151473\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "clf = BalancedRandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "print(eval_on_original(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_tuning():\n",
    "    model = XGBClassifier(\n",
    "        tree_method=\"gpu_hist\",\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=7.133,\n",
    "        eval_metric=\"logloss\", \n",
    "    )\n",
    "\n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"reg_alpha\", \"learning_rate\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "    \n",
    "    pbounds = {\n",
    "        \"max_depth\": (2, 25),\n",
    "        \"min_child_weight\": (0, 20)\n",
    "    }\n",
    "\n",
    "    return tuning_super_learner(model, pbounds, par_mod=par_mod, n=5, init_points=20, n_iter=40)\n",
    "superBO = super_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876288195422061\n"
     ]
    }
   ],
   "source": [
    "from models import MODEL_LIST, TRANSFORM_LIST\n",
    "\n",
    "# model_name = \"xgb\"\n",
    "# model = MODEL_LIST[model_name][\"model\"]\n",
    "# model.set_params(**MODEL_LIST[model_name][\"params\"])\n",
    "# data_params = TRANSFORM_LIST[model_name]\n",
    "\n",
    "# model.set_params(n_estimators=1000, learning_rate=0.1)\n",
    "\n",
    "\n",
    "print(eval_on_original(model,  targets=\"risk_flag\", data_params=data_params, fit_params={}, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.7725611149188942 (RF)\n",
    "- 0.8480793898917687 (LGBM)\n",
    "- 0.9994960751423004\n",
    "- 0.9996103086355607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Parameters\n",
    "---\n",
    "# XGBoost\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(\n",
    "    tree_method=\"gpu_hist\", \n",
    "    learning_rate=0.00494, \n",
    "    n_estimators=10000, \n",
    "    eval_metric=\"logloss\", \n",
    "    scale_pos_weight=19.35097,\n",
    "    max_depth=8,\n",
    "    min_child_weight=4.525,\n",
    "    gamma=0.938358\n",
    ")\n",
    "```\n",
    "---\n",
    "# LightGBM\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    learning_rate=0.005,\n",
    "    n_estimators=10000,\n",
    "    device=\"gpu\",\n",
    "    boosting_type=\"goss\",\n",
    "    num_leaves=229,\n",
    "    scale_pos_weight=13.56929,\n",
    "    colsample_bytree=0.95781,\n",
    "    subsample=0.74331,\n",
    "    max_depth=7,\n",
    "    min_child_weight=7.5546,\n",
    "    max_bin=96,\n",
    "    lambda_l1=0.0868,\n",
    "    lambda_l2=0.01541,\n",
    "    verbose=-1\n",
    ")\n",
    "```\n",
    "---\n",
    "# CatBoost\n",
    "\n",
    "```python\n",
    "cat_features = [\"house_ownership\", \"car_ownership\", \"married\", \"city\", \"profession\", \"state\", \"age\", \"experience\", \"income\"]\n",
    "model = CatBoostClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    loss_function=\"Logloss\",\n",
    "    boosting_type=\"Ordered\",\n",
    "    eval_metric=\"AUC\",\n",
    "    cat_features=cat_features,\n",
    "    max_depth=5,\n",
    "    task_type=\"GPU\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "data_params = {\"drop_cols\": {}, \"transforms\": [(catboost_dataset, {\"cols\": cat_features})], \"no_encode\": True}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9934563320481204, 0.00027806230471899107]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import *\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# model = XGBClassifier(\n",
    "#     tree_method=\"gpu_hist\", \n",
    "#     learning_rate=0.1, \n",
    "#     n_estimators=500, \n",
    "#     eval_metric=\"logloss\", \n",
    "#     max_depth=8,\n",
    "#     min_child_weight=4.525,\n",
    "#     gamma=0.938358\n",
    "# )\n",
    "model = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_features=1)\n",
    "df_train, df_test = data_preprocess()\n",
    "\n",
    "under = InstanceHardnessThreshold(sampling_strategy=1, n_jobs=-1)\n",
    "over = SMOTE(sampling_strategy=0.75, k_neighbors=5)\n",
    "sampler = Pipeline(steps=[('o', over), ('u', under)])\n",
    "\n",
    "res = []\n",
    "for _ in range(1):\n",
    "    X, y = sampler.fit_resample(df_train.drop([\"id\", \"risk_flag\"], axis=1), df_train[\"risk_flag\"].values)\n",
    "    model.fit(X, y)\n",
    "    preds = model.predict(df_test.drop(\"id\", axis=1))\n",
    "    res.append(evaluate_predictions(preds, \"./BEST.csv\"))\n",
    "res = np.array(res)\n",
    "print([res.mean(), res.std()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling Only\n",
    "---\n",
    "- 0.868648 (no sampling)\n",
    "- 0.718138 (0.5)\n",
    "- 0.780676 (0.8)\n",
    "- 0.798733 (1.0)\n",
    "\n",
    "## Oversampling Only\n",
    "---\n",
    "- 0.776018 (`sampling_strategy`=0.5)\n",
    "- 0.794863 (`sampling_strategy`=0.8)\n",
    "- 0.858355 (`sampling_strategy`=0.75, `k_neighbors`=5) (`InstanceHardnessThreshold`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
