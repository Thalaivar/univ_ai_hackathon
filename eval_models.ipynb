{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import catboost as cboost\n",
    "\n",
    "from functools import partial\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(preds, best_file):\n",
    "    return roc_auc_score(pd.read_csv(best_file)[\"risk_flag\"].values, preds)\n",
    "\n",
    "def eval_on_original(model, targets=\"risk_flag\", data_params={}, fit_params={}, par_mod=None, n=1, **params):\n",
    "    model = clone(model)\n",
    "    \n",
    "    if par_mod:\n",
    "        params = par_mod(params)\n",
    "    model.set_params(**params)\n",
    "    \n",
    "    df_train, df_test = data_preprocess(**data_params)\n",
    "    \n",
    "    res = []\n",
    "    for _ in range(n):\n",
    "        X, y = df_train.drop([\"id\", targets], axis=1), df_train[targets].values\n",
    "        X, y = AllKNN(n_jobs=-1, n_neighbors=20).fit_resample(X, y)\n",
    "        model.fit(X, y, **fit_params)\n",
    "    \n",
    "        preds = model.predict(df_test.drop(\"id\", axis=1))\n",
    "        res.append(evaluate_predictions(preds, \"./BEST.csv\"))\n",
    "        \n",
    "    res = np.array(res)\n",
    "    return res.mean()\n",
    "\n",
    "\n",
    "def tuning_w_original(model, pbounds, targets=\"risk_flag\", data_params={}, fit_params={}, par_mod=None, **opt_kwargs):\n",
    "    eval_fn = partial(eval_on_original, model, targets, data_params, fit_params, par_mod)\n",
    "    BO = BayesianOptimization(eval_fn, pbounds)\n",
    "    BO.maximize(**opt_kwargs)\n",
    "    return BO\n",
    "\n",
    "def tuning_super_learner(model, pbounds, fit_params={}, par_mod=None, n=5, **opt_kwargs):\n",
    "    df_train = pd.read_csv(\"./ensemble_files/ensemble_train.csv\")\n",
    "    df_test = pd.read_csv(\"./ensemble_files/ensemble_test.csv\")\n",
    "\n",
    "    def eval_fn(model, df_train, df_test, fit_params, par_mod, **params):\n",
    "        model = clone(model)\n",
    "        params = par_mod(params)\n",
    "        model.set_params(**params)\n",
    "\n",
    "        res = []\n",
    "        for _ in range(n):\n",
    "            model.fit(df_train.drop([\"id\", \"targets\"], axis=1), df_train[\"targets\"].values, **fit_params)\n",
    "            preds = model.predict(df_test.drop(\"id\", axis=1))\n",
    "            res.append(evaluate_predictions(preds, \"./BEST.csv\"))\n",
    "        \n",
    "        \n",
    "    \n",
    "    eval_fn = partial(eval_fn, model, df_train, df_test, fit_params, par_mod)\n",
    "    superBO = BayesianOptimization(eval_fn, pbounds)\n",
    "    superBO.maximize(**opt_kwargs)\n",
    "    return superBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning on Original Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xgb_tuning():\n",
    "    model = XGBClassifier(\n",
    "        tree_method=\"gpu_hist\", \n",
    "        n_estimators=200,\n",
    "        eval_metric=\"logloss\",\n",
    "        learning_rate=0.1,\n",
    "        max_depth=23,\n",
    "        min_child_weight=3.481\n",
    "    )\n",
    "\n",
    "    pbounds = {\n",
    "        \"gamma\": (0, 1),\n",
    "        \"reg_alpha\": (-5, 1)\n",
    "    }\n",
    "    \n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"reg_alpha\", \"learning_rate\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "    \n",
    "    data_params = {\"dtrain\": pd.read_csv(\"./train.csv\"), \"dtest\": pd.read_csv(\"./test.csv\")}\n",
    "    return tuning_w_original(model, pbounds, par_mod=par_mod, data_params=data_params, init_points=20, n_iter=40)\n",
    "\n",
    "xgbBO = xgb_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbBO.max[\"params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_tuning(model=None, pbounds=None, init_params=None):\n",
    "    if model is None:\n",
    "        model = LGBMClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=200,\n",
    "            device=\"gpu\",\n",
    "            num_leaves=256,\n",
    "            boosting_type=\"goss\",\n",
    "            max_depth=23,\n",
    "            max_bin=64,\n",
    "            min_child_weight=1.899,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\", \"max_bin\", \"num_leaves\", \"bagging_freq\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"lambda_l1\", \"lambda_l2\", \"learning_rate\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "    \n",
    "    if init_params is not None:\n",
    "        model.set_params(**par_mod(init_params))\n",
    "\n",
    "    if pbounds is None:\n",
    "        pbounds = {\n",
    "            \"lambda_l1\": (-5, 1),\n",
    "            \"lambda_l2\": (-5, 1)\n",
    "        }\n",
    "    \n",
    "    data_params = {\"dtrain\": pd.read_csv(\"./train.csv\"), \"dtest\": pd.read_csv(\"./test.csv\")}\n",
    "    return tuning_w_original(model, pbounds, par_mod=par_mod, data_params=data_params, init_points=20, n_iter=40)\n",
    "\n",
    "lgbBO = lgbm_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_tuning():\n",
    "    cat_features = [\"house_ownership\", \"car_ownership\", \"married\", \"city\", \"profession\", \"state\", \"age\", \"experience\", \"income\"]\n",
    "    model = CatBoostClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        loss_function=\"Logloss\",\n",
    "        boosting_type=\"Ordered\",\n",
    "        eval_metric=\"AUC\",\n",
    "        cat_features=cat_features,\n",
    "        task_type=\"GPU\",\n",
    "        max_depth=5,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\", \"max_bin\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"l2_leaf_reg\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "   \n",
    "    pbounds = {\n",
    "#         \"random_strength\": (0, 20),\n",
    "#         \"subsample\": (0.1, 1),\n",
    "#         \"scale_pos_weight\": (5, 20),\n",
    "#         \"l2_leaf_reg\": (-5, 1),\n",
    "#         \"bagging_temperature\": (0.1, 1)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    data_params = {\"drop_cols\": {}, \"transforms\": [(catboost_dataset, {\"cols\": cat_features})], \"no_encode\": True}\n",
    "    return tuning_w_original(model, pbounds, par_mod=par_mod, data_params=data_params, init_points=20, n_iter=40)\n",
    "cboostBO = catboost_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "model = XGBClassifier(\n",
    "        tree_method=\"gpu_hist\", \n",
    "        n_estimators=3000,\n",
    "        eval_metric=\"logloss\",\n",
    "        learning_rate=0.1,\n",
    "        max_depth=23,\n",
    "        min_child_weight=3.481,\n",
    "        gamma=0,\n",
    "        reg_alpha=10 ** -3.4825\n",
    "    )\n",
    "print(eval_on_original(model, data_params={\"dtrain\": pd.read_csv(\"./train.csv\"), \"dtest\": pd.read_csv(\"./test.csv\")}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_test' referenced before assignment",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0d8f955aa1d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"risk_flag\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"risk_flag\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAllKNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\univ_ai_hackathon\\models.py\u001b[0m in \u001b[0;36mdata_preprocess\u001b[1;34m(dtrain, dtest, drop_cols, no_encode)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"id\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"profession\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"city\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"profession\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"city\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'df_test' referenced before assignment"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"tree_method\": \"gpu_hist\", \n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_depth\": 23,\n",
    "    \"min_child_weight\": 3.481,\n",
    "    \"gamma\": 0,\n",
    "    \"reg_alpha\": 10 ** -3.4825\n",
    "}\n",
    "\n",
    "df_train = data_preprocess(pd.read_csv(\"./train.csv\"))\n",
    "X, y = df_train.drop([\"id\", \"risk_flag\"], axis=1), df_train[\"risk_flag\"].values\n",
    "X, y = AllKNN(n_jobs=-1, n_neighbors=20).fit_resample(X, y)\n",
    "dtrain = xgb.DMatrix(data=X, label=y)\n",
    "res = xgb.cv(params, dtrain, 10000, metrics=\"auc\", nfold=5, stratified=True, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_tuning():\n",
    "    model = XGBClassifier(\n",
    "        tree_method=\"gpu_hist\",\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=7.133,\n",
    "        eval_metric=\"logloss\", \n",
    "    )\n",
    "\n",
    "    def par_mod(params):\n",
    "        for par in [\"max_depth\"]:\n",
    "            if par in params:\n",
    "                params[par] = int(round(params[par]))\n",
    "        for par in [\"reg_alpha\", \"learning_rate\"]:\n",
    "            if par in params:\n",
    "                params[par] = 10 ** params[par]\n",
    "        return params\n",
    "    \n",
    "    pbounds = {\n",
    "        \"max_depth\": (2, 25),\n",
    "        \"min_child_weight\": (0, 20)\n",
    "    }\n",
    "\n",
    "    return tuning_super_learner(model, pbounds, par_mod=par_mod, n=5, init_points=20, n_iter=40)\n",
    "superBO = super_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import MODEL_LIST, TRANSFORM_LIST\n",
    "\n",
    "# model_name = \"xgb\"\n",
    "# model = MODEL_LIST[model_name][\"model\"]\n",
    "# model.set_params(**MODEL_LIST[model_name][\"params\"])\n",
    "# data_params = TRANSFORM_LIST[model_name]\n",
    "\n",
    "# model.set_params(n_estimators=1000, learning_rate=0.1)\n",
    "\n",
    "model = LGBMClassifier(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=2000,\n",
    "            device=\"gpu\",\n",
    "            num_leaves=2 ** 16,\n",
    "            boosting_type=\"goss\",\n",
    "            max_depth=23,\n",
    "            max_bin=64,\n",
    "            min_child_weight=1.899,\n",
    "            lambda_l1=10**-3.565,\n",
    "            lambda_l2=10**-2.953,\n",
    "            verbose=-1\n",
    "        )\n",
    "data_params = {\"dtrain\": pd.read_csv(\"./train.csv\"), \"dtest\": pd.read_csv(\"./test.csv\")}\n",
    "print(eval_on_original(model,  targets=\"risk_flag\", data_params=data_params, fit_params={}, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10 **-3.565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Parameters\n",
    "---\n",
    "# XGBoost\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(\n",
    "    tree_method=\"gpu_hist\", \n",
    "    learning_rate=0.00494, \n",
    "    n_estimators=10000, \n",
    "    eval_metric=\"logloss\", \n",
    "    scale_pos_weight=19.35097,\n",
    "    max_depth=8,\n",
    "    min_child_weight=4.525,\n",
    "    gamma=0.938358\n",
    ")\n",
    "```\n",
    "---\n",
    "# LightGBM\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    learning_rate=0.005,\n",
    "    n_estimators=10000,\n",
    "    device=\"gpu\",\n",
    "    boosting_type=\"goss\",\n",
    "    num_leaves=229,\n",
    "    scale_pos_weight=13.56929,\n",
    "    colsample_bytree=0.95781,\n",
    "    subsample=0.74331,\n",
    "    max_depth=7,\n",
    "    min_child_weight=7.5546,\n",
    "    max_bin=96,\n",
    "    lambda_l1=0.0868,\n",
    "    lambda_l2=0.01541,\n",
    "    verbose=-1\n",
    ")\n",
    "```\n",
    "---\n",
    "# CatBoost\n",
    "\n",
    "```python\n",
    "cat_features = [\"house_ownership\", \"car_ownership\", \"married\", \"city\", \"profession\", \"state\", \"age\", \"experience\", \"income\"]\n",
    "model = CatBoostClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    loss_function=\"Logloss\",\n",
    "    boosting_type=\"Ordered\",\n",
    "    eval_metric=\"AUC\",\n",
    "    cat_features=cat_features,\n",
    "    max_depth=5,\n",
    "    task_type=\"GPU\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "data_params = {\"drop_cols\": {}, \"transforms\": [(catboost_dataset, {\"cols\": cat_features})], \"no_encode\": True}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "model =  XGBClassifier(\n",
    "                    n_estimators=3000,\n",
    "                    learning_rate=0.01,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    tree_method=\"gpu_hist\",\n",
    "                    # gamma=0.9384,\n",
    "                    # max_depth=8,\n",
    "                    # min_child_weight=4.525\n",
    "                )\n",
    "def lgbv1_transform_fn(df, target_name=\"risk_flag\"):\n",
    "    sep = TomekLinks(sampling_strategy=\"not minority\", n_jobs=-1)\n",
    "    # under = InstanceHardnessThreshold(sampling_strategy=1, n_jobs=-1)\n",
    "    sampler = Pipeline(steps=[('u1', sep)])\n",
    "    X, y = df.drop([\"id\", target_name], axis=1), df[target_name].values\n",
    "    # X, y = sampler.fit_resample(X, y)\n",
    "    return X, y\n",
    "\n",
    "res = []\n",
    "for _ in range(1):\n",
    "    dtrain, dtest = data_preprocess(pd.read_csv(\"./train.csv\"), pd.read_csv(\"./test.csv\"))\n",
    "    Xtrain, ytrain = lgbv1_transform_fn(dtrain)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    preds = model.predict(dtest.drop(\"id\", axis=1))\n",
    "    res.append(evaluate_predictions(preds, \"./BEST.csv\"))\n",
    "res = np.array(res)\n",
    "print([res.mean(), res.std()])"
   ]
  },
  {
   "source": [
    "- 0.6753484121544437"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd00bda99ab821e2c1486c14791a092059d9b5f1492bb5b2b817e20afe3617e9aa8",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}